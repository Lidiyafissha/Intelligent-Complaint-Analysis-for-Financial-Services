# ğŸ§ Intelligent-Complaint-Analysis-for-Financial-Services

A Retrieval-Augmented Generation (RAG) Pipeline for Semantic Complaint Search


ğŸ“Œ Project Overview

Financial institutions receive thousands of consumer complaints every day.
Hidden within these narratives are patterns, pain points, and signals that can guide better decisionsâ€”but only if the text can be searched and understood intelligently.

This project builds the **foundation of a Retrieval-Augmented Generation (RAG) system** by transforming raw consumer complaint narratives into a **semantic searchâ€“ready vector database**.
The result is a clean, reproducible pipeline that prepares complaint data for downstream applications such as chatbots, question-answering systems, and analytics tools.

The work is divided into two main stages:

1. **Exploratory Data Analysis & Preprocessing**
2. **Text Chunking, Embedding, and Vector Store Indexing**

---

 ğŸ—‚ Project Structure

```text
rag-complaint-chatbot/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Original CFPB complaint dataset
â”‚   â””â”€â”€ processed/               # Cleaned and filtered complaint data
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ task1_eda_preprocessing.ipynb
â”‚   â””â”€â”€ task2_embedding_pipeline.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                # Central configuration (columns, seeds, paths)
â”‚   â”œâ”€â”€ utils.py                 # Validation and helper utilities
â”‚   â”œâ”€â”€ sampling.py              # Stratified sampling logic
â”‚   â”œâ”€â”€ chunking.py              # Text chunking functions
â”‚   â”œâ”€â”€ embeddings.py            # Embedding model loader
â”‚   â””â”€â”€ vector_store.py          # FAISS/ChromaDB indexing logic
â”‚
â”œâ”€â”€ vector_store/                # Persisted vector database
â”œâ”€â”€ app.py                       # (Optional) UI layer for future use
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

This structure separates **logic (src/)** from **execution (notebooks/)**, ensuring clarity, reusability, and reproducibility.

---

## ğŸ§ª Task 1: Exploratory Data Analysis & Preprocessing

### ğŸ¯ Objective

Understand the structure and quality of the CFPB complaint data and prepare it for semantic embedding.

### ğŸ” Key Steps

1. **Dataset Loading**

   * Loaded the full CFPB complaint dataset from `data/raw/`.

2. **Initial EDA**

   * Analyzed complaint distribution across financial products.
   * Examined the presence and absence of complaint narratives.
   * Calculated word counts to identify very short or very long narratives.

3. **Narrative Length Analysis**

   * Visualized word count distribution.
   * Confirmed the presence of extremely short narratives (low information value) and very long narratives (requiring chunking).

4. **Filtering**

   * Retained complaints from five target product categories.
   * Removed records with empty or missing complaint narratives.

5. **Text Cleaning**

   * Converted text to lowercase.
   * Removed special characters and boilerplate phrases.
   * Prepared narratives for higher-quality embeddings.

### ğŸ“¦ Output

* Cleaned dataset saved as:

  ```
  data/processed/filtered_complaints.csv
  ```

---

## ğŸ§© Task 2: Text Chunking, Embedding, and Vector Store Indexing

### ğŸ¯ Objective

Convert cleaned complaint narratives into a format suitable for **efficient semantic search**.

---

### ğŸ“Š Stratified Sampling

* Created a **stratified sample of 10,000â€“15,000 complaints**.
* Ensured proportional representation across all product categories.
* Used a fixed random seed for reproducibility.
* Sampling logic implemented in `src/sampling.py`.

This prevents dominant products from overshadowing smaller categories.

---

### âœ‚ï¸ Text Chunking Strategy

Long complaint narratives were split into smaller, overlapping chunks to improve embedding quality.

* Implemented using:

  * Custom chunking logic (fallback-safe)
  * Optional LangChain `RecursiveCharacterTextSplitter`
* Final configuration:

  * `chunk_size`: balances semantic coherence
  * `chunk_overlap`: preserves contextual continuity

Chunking logic lives in `src/chunking.py`.

---

### ğŸ§  Embedding Model Choice

**Model used:**
`sentence-transformers/all-MiniLM-L6-v2`

**Why this model?**

* Strong semantic performance
* Lightweight and fast
* Well-suited for sentence-level embeddings
* Widely adopted and well-documented

Embedding loading is handled in `src/embeddings.py`.

---

### ğŸ—ƒ Vector Store Indexing

* Generated embeddings for each text chunk.
* Stored vectors using **FAISS** (or ChromaDB as a fallback).
* Persisted the vector store to disk.

Each vector includes **metadata**, such as:

* Complaint ID
* Product category
* Chunk index

This ensures every retrieved chunk can be traced back to its original source.

Indexing logic is implemented in `src/vector_store.py`.

---

## â™»ï¸ Reproducibility & Robustness

This project was designed to be **robust and reproducible**:

* âœ… Central configuration in `src/config.py`
* âœ… Fixed random seeds for sampling
* âœ… Input validation with clear error messages
* âœ… Graceful fallbacks for missing libraries
* âœ… Modular design for easy extension

All modules are orchestrated from a **single notebook**, ensuring clarity while maintaining clean separation of concerns.

---

## ğŸš€ How to Run

1. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

2. Run notebooks in order:

   * `notebooks/eda1.ipynb`
   * `notebooks/final_eda.ipynb`

3. The persisted vector store will be available in:

   ```
   vector_store/
   ```

---

## ğŸŒ± Future Work

* Integrate a RAG chatbot interface (Gradio or Streamlit)
* Add evaluation for retrieval quality
* Experiment with larger embedding models
* Support real-time complaint ingestion

---

## âœ¨ Closing Note

This project lays a strong, thoughtful foundation for intelligent complaint analysis.
Each stepâ€”EDA, cleaning, sampling, chunking, embedding, and indexingâ€”was designed with care, clarity, and purpose.


